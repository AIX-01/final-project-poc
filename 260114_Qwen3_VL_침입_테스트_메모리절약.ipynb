{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9AmUNdYf5FcW","executionInfo":{"status":"ok","timestamp":1768384743583,"user_tz":-540,"elapsed":15645,"user":{"displayName":"다래","userId":"06334432749296783356"}},"outputId":"2c99e6f0-eda5-4692-8572-e6d9973705e1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vDYxRjlmbxNp"},"outputs":[],"source":["%%capture\n","import os, re\n","if \"COLAB_\" not in \"\".join(os.environ.keys()):\n","    !pip install unsloth\n","else:\n","    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n","    import torch; v = re.match(r\"[0-9]{1,}\\.[0-9]{1,}\", str(torch.__version__)).group(0)\n","    xformers = \"xformers==\" + (\"0.0.33.post1\" if v==\"2.9\" else \"0.0.32.post2\" if v==\"2.8\" else \"0.0.29.post3\")\n","    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n","    !pip install sentencepiece protobuf \"datasets==4.3.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n","    !pip install --no-deps unsloth\n","!pip install transformers==4.57.1\n","!pip install --no-deps trl==0.22.2"]},{"cell_type":"code","source":["from unsloth import FastVisionModel # FastLanguageModel for LLMs\n","import torch\n","\n","#model_id = \"unsloth/Qwen3-VL-2B-Thinking-unsloth-bnb-4bit\"\n","model_id = \"unsloth/Qwen3-VL-2B-Instruct-unsloth-bnb-4bit\"\n","model, tokenizer = FastVisionModel.from_pretrained(\n","    model_id,\n","    load_in_4bit = True, # Use 4bit to reduce memory use. False for 16bit LoRA.\n","    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for long context\n",")"],"metadata":{"id":"BiTr_JGpcswP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1768371710534,"user_tz":-540,"elapsed":27947,"user":{"displayName":"다래","userId":"06334432749296783356"}},"outputId":"b245d1e5-62d8-44a2-82b8-d898e348010d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["==((====))==  Unsloth 2026.1.2: Fast Qwen3_Vl patching. Transformers: 4.57.1.\n","   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n","O^O/ \\_/ \\    Torch: 2.9.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.5.0\n","\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.33.post1. FA2 = False]\n"," \"-____-\"     Free license: http://github.com/unslothai/unsloth\n","Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"]}]},{"cell_type":"code","source":["from transformers import AutoProcessor\n","\n","# 보통 processor는 HF 쪽을 쓰는 게 편함 (멀티이미지 입력)\n","processor = AutoProcessor.from_pretrained(model_id)\n","FastVisionModel.for_inference(model)  # inference 최적화 (unsloth 스타일)"],"metadata":{"id":"-_GYE2kldDBU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1768371713675,"user_tz":-540,"elapsed":3070,"user":{"displayName":"다래","userId":"06334432749296783356"}},"outputId":"c09d81ce-f505-4c1b-fef1-e09c74af43ed"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Qwen3VLForConditionalGeneration(\n","  (model): Qwen3VLModel(\n","    (visual): Qwen3VLVisionModel(\n","      (patch_embed): Qwen3VLVisionPatchEmbed(\n","        (proj): Conv3d(3, 1024, kernel_size=(2, 16, 16), stride=(2, 16, 16))\n","      )\n","      (pos_embed): Embedding(2304, 1024)\n","      (rotary_pos_emb): Qwen3VLVisionRotaryEmbedding()\n","      (blocks): ModuleList(\n","        (0-23): 24 x Qwen3VLVisionBlock(\n","          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n","          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n","          (attn): Qwen3VLVisionAttention(\n","            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n","            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n","          )\n","          (mlp): Qwen3VLVisionMLP(\n","            (linear_fc1): Linear(in_features=1024, out_features=4096, bias=True)\n","            (linear_fc2): Linear(in_features=4096, out_features=1024, bias=True)\n","            (act_fn): GELUTanh()\n","          )\n","        )\n","      )\n","      (merger): Qwen3VLVisionPatchMerger(\n","        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n","        (linear_fc1): Linear(in_features=4096, out_features=4096, bias=True)\n","        (act_fn): GELU(approximate='none')\n","        (linear_fc2): Linear(in_features=4096, out_features=2048, bias=True)\n","      )\n","      (deepstack_merger_list): ModuleList(\n","        (0-2): 3 x Qwen3VLVisionPatchMerger(\n","          (norm): LayerNorm((4096,), eps=1e-06, elementwise_affine=True)\n","          (linear_fc1): Linear(in_features=4096, out_features=4096, bias=True)\n","          (act_fn): GELU(approximate='none')\n","          (linear_fc2): Linear(in_features=4096, out_features=2048, bias=True)\n","        )\n","      )\n","    )\n","    (language_model): Qwen3VLTextModel(\n","      (embed_tokens): Embedding(151936, 2048)\n","      (layers): ModuleList(\n","        (0): Qwen3VLTextDecoderLayer(\n","          (self_attn): Qwen3VLTextAttention(\n","            (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n","            (k_proj): Linear(in_features=2048, out_features=1024, bias=False)\n","            (v_proj): Linear(in_features=2048, out_features=1024, bias=False)\n","            (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n","            (q_norm): Qwen3VLTextRMSNorm((128,), eps=1e-06)\n","            (k_norm): Qwen3VLTextRMSNorm((128,), eps=1e-06)\n","          )\n","          (mlp): Qwen3VLTextMLP(\n","            (gate_proj): Linear(in_features=2048, out_features=6144, bias=False)\n","            (up_proj): Linear(in_features=2048, out_features=6144, bias=False)\n","            (down_proj): Linear(in_features=6144, out_features=2048, bias=False)\n","            (act_fn): SiLUActivation()\n","          )\n","          (input_layernorm): Qwen3VLTextRMSNorm((2048,), eps=1e-06)\n","          (post_attention_layernorm): Qwen3VLTextRMSNorm((2048,), eps=1e-06)\n","        )\n","        (1): Qwen3VLTextDecoderLayer(\n","          (self_attn): Qwen3VLTextAttention(\n","            (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n","            (k_proj): Linear4bit(in_features=2048, out_features=1024, bias=False)\n","            (v_proj): Linear4bit(in_features=2048, out_features=1024, bias=False)\n","            (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n","            (q_norm): Qwen3VLTextRMSNorm((128,), eps=1e-06)\n","            (k_norm): Qwen3VLTextRMSNorm((128,), eps=1e-06)\n","          )\n","          (mlp): Qwen3VLTextMLP(\n","            (gate_proj): Linear4bit(in_features=2048, out_features=6144, bias=False)\n","            (up_proj): Linear4bit(in_features=2048, out_features=6144, bias=False)\n","            (down_proj): Linear4bit(in_features=6144, out_features=2048, bias=False)\n","            (act_fn): SiLUActivation()\n","          )\n","          (input_layernorm): Qwen3VLTextRMSNorm((2048,), eps=1e-06)\n","          (post_attention_layernorm): Qwen3VLTextRMSNorm((2048,), eps=1e-06)\n","        )\n","        (2-3): 2 x Qwen3VLTextDecoderLayer(\n","          (self_attn): Qwen3VLTextAttention(\n","            (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n","            (k_proj): Linear4bit(in_features=2048, out_features=1024, bias=False)\n","            (v_proj): Linear4bit(in_features=2048, out_features=1024, bias=False)\n","            (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n","            (q_norm): Qwen3VLTextRMSNorm((128,), eps=1e-06)\n","            (k_norm): Qwen3VLTextRMSNorm((128,), eps=1e-06)\n","          )\n","          (mlp): Qwen3VLTextMLP(\n","            (gate_proj): Linear(in_features=2048, out_features=6144, bias=False)\n","            (up_proj): Linear(in_features=2048, out_features=6144, bias=False)\n","            (down_proj): Linear(in_features=6144, out_features=2048, bias=False)\n","            (act_fn): SiLUActivation()\n","          )\n","          (input_layernorm): Qwen3VLTextRMSNorm((2048,), eps=1e-06)\n","          (post_attention_layernorm): Qwen3VLTextRMSNorm((2048,), eps=1e-06)\n","        )\n","        (4-26): 23 x Qwen3VLTextDecoderLayer(\n","          (self_attn): Qwen3VLTextAttention(\n","            (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n","            (k_proj): Linear4bit(in_features=2048, out_features=1024, bias=False)\n","            (v_proj): Linear4bit(in_features=2048, out_features=1024, bias=False)\n","            (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n","            (q_norm): Qwen3VLTextRMSNorm((128,), eps=1e-06)\n","            (k_norm): Qwen3VLTextRMSNorm((128,), eps=1e-06)\n","          )\n","          (mlp): Qwen3VLTextMLP(\n","            (gate_proj): Linear4bit(in_features=2048, out_features=6144, bias=False)\n","            (up_proj): Linear4bit(in_features=2048, out_features=6144, bias=False)\n","            (down_proj): Linear4bit(in_features=6144, out_features=2048, bias=False)\n","            (act_fn): SiLUActivation()\n","          )\n","          (input_layernorm): Qwen3VLTextRMSNorm((2048,), eps=1e-06)\n","          (post_attention_layernorm): Qwen3VLTextRMSNorm((2048,), eps=1e-06)\n","        )\n","        (27): Qwen3VLTextDecoderLayer(\n","          (self_attn): Qwen3VLTextAttention(\n","            (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n","            (k_proj): Linear4bit(in_features=2048, out_features=1024, bias=False)\n","            (v_proj): Linear4bit(in_features=2048, out_features=1024, bias=False)\n","            (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n","            (q_norm): Qwen3VLTextRMSNorm((128,), eps=1e-06)\n","            (k_norm): Qwen3VLTextRMSNorm((128,), eps=1e-06)\n","          )\n","          (mlp): Qwen3VLTextMLP(\n","            (gate_proj): Linear(in_features=2048, out_features=6144, bias=False)\n","            (up_proj): Linear(in_features=2048, out_features=6144, bias=False)\n","            (down_proj): Linear(in_features=6144, out_features=2048, bias=False)\n","            (act_fn): SiLUActivation()\n","          )\n","          (input_layernorm): Qwen3VLTextRMSNorm((2048,), eps=1e-06)\n","          (post_attention_layernorm): Qwen3VLTextRMSNorm((2048,), eps=1e-06)\n","        )\n","      )\n","      (norm): Qwen3VLTextRMSNorm((2048,), eps=1e-06)\n","      (rotary_emb): Qwen3VLTextRotaryEmbedding()\n","    )\n","  )\n","  (lm_head): Linear(in_features=2048, out_features=151936, bias=False)\n",")"]},"metadata":{},"execution_count":27}]},{"cell_type":"code","source":["def show_thumb_grid(\n","    images,\n","    scale=0.10,      # 10% 축소\n","    cols=4,          # 4x2 (8프레임 기준)\n","    gap=6,           # 썸네일 간격(px)\n","    bg=(18, 18, 18)  # 배경색\n","):\n","    \"\"\"\n","    images: List[PIL.Image.Image]\n","    scale : 0~1 (원본 대비 축소 비율)\n","    cols  : 그리드 열 수\n","    gap   : 타일 간격\n","    \"\"\"\n","    if not images:\n","        return\n","\n","    # 1) 썸네일 생성\n","    thumbs = []\n","    for im in images:\n","        w, h = im.size\n","        tw, th = max(1, int(w * scale)), max(1, int(h * scale))\n","        thumbs.append(im.resize((tw, th), resample=Image.BILINEAR))\n","\n","    # 2) 그리드 캔버스 만들기\n","    rows = (len(thumbs) + cols - 1) // cols\n","    cell_w = max(t.size[0] for t in thumbs)\n","    cell_h = max(t.size[1] for t in thumbs)\n","\n","    grid_w = cols * cell_w + (cols - 1) * gap\n","    grid_h = rows * cell_h + (rows - 1) * gap\n","\n","    grid = Image.new(\"RGB\", (grid_w, grid_h), bg)\n","\n","    # 3) paste\n","    for idx, t in enumerate(thumbs):\n","        r = idx // cols\n","        c = idx % cols\n","        x = c * (cell_w + gap)\n","        y = r * (cell_h + gap)\n","        grid.paste(t, (x, y))\n","\n","    display(grid)\n","    return grid  # 필요하면 저장/추가 처리 가능"],"metadata":{"id":"ul8Fr7-jmzWr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","영상 크기가 큰 경우(700mb 이상) ram 절약하기 위해 개선한 코드임\n","\n","코드 수정한 프롬프트(제미나이 2.5 플래시):\n","마지막 셀 코드에 리스트에 frames을 리스트 저장이 아닌 yield 하고, 아래 for문도 generator를 인자값으로 받아서 실행할 수 있게 함수화 및 실행 코드로 바꿔줘, ram 절약 관점에서만 개선해주는 \"리팩토링\"이 되어야 해\n","\"\"\"\n","import cv2\n","from PIL import Image\n","from IPython.display import display\n","import time\n","from datetime import datetime\n","import torch\n","\n","# Helper functions\n","def ts():\n","    return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n","\n","def sync():\n","    if torch.cuda.is_available():\n","        torch.cuda.synchronize()\n","\n","\n","def frame_generator(video_path, max_seconds=None):\n","    cap = cv2.VideoCapture(video_path)\n","    if not cap.isOpened():\n","        raise RuntimeError(f\"Failed to open video: {video_path}\")\n","\n","    fps = cap.get(cv2.CAP_PROP_FPS)\n","    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","    duration_sec = int(total_frames / fps) if fps and fps > 0 else None\n","\n","    sec = 0\n","    while True:\n","        if max_seconds is not None and sec >= max_seconds:\n","            break\n","        if duration_sec is not None and sec >= duration_sec:\n","            break\n","\n","        cap.set(cv2.CAP_PROP_POS_MSEC, sec * 1000.0)\n","        ok, bgr = cap.read()\n","        if not ok:\n","            break\n","\n","        rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n","        yield Image.fromarray(rgb)\n","        sec += 1\n","    cap.release()\n","\n","def process_video_frames(frame_gen, chunk_size, max_new_tokens, processor, model):\n","    prev_summary = None\n","    t_all0 = time.perf_counter()\n","    print(f\"[{ts()}] Start total\")\n","\n","    chunk_buffer = []\n","    current_frame_idx = 0\n","\n","    for frame in frame_gen:\n","        chunk_buffer.append(frame)\n","        current_frame_idx += 1\n","\n","        if len(chunk_buffer) == chunk_size:\n","            t_chunk0 = time.perf_counter()\n","            start_s = current_frame_idx - chunk_size\n","            end_s = current_frame_idx - 1\n","            print(f\"\\n[{ts()}] === Chunk {start_s}s ~ {end_s}s START ===\")\n","\n","            chunk = chunk_buffer\n","\n","            show_thumb_grid(chunk, scale=0.10, cols=4)\n","\n","            resized_chunk = []\n","            target_w = 512\n","            for im in chunk:\n","                w, h = im.size\n","                if w > target_w:\n","                    new_h = int(h * (target_w / w))\n","                    im = im.resize((target_w, new_h))\n","                resized_chunk.append(im)\n","\n","            chunk = resized_chunk\n","\n","            t0 = time.perf_counter()\n","            content = [{\"type\": \"image\", \"image\": im} for im in chunk]\n","            t1 = time.perf_counter()\n","            print(f\"[{ts()}] step2 build_image_content: {(t1 - t0)*1000:.1f} ms\")\n","\n","            t0 = time.perf_counter()\n","            prompt = (\n","                f\"These are frames sampled at 1 FPS from {start_s}s to {end_s}s.\\n\"\n","                \"Write a concise 2–3 sentence summary in English.\\n\"\n","                \"Do NOT output any reasoning, analysis, chain-of-thought, or step-by-step thinking.\\n\"\n","                \"Only output the final summary text.\\n\"\n","                \"If a new character appears, briefly describe appearance and actions.\\n\"\n","                \"If the character already appeared, refer to the prior description.\\n\"\n","                \"Keep it under 70 words.\"\n","            )\n","            if prev_summary:\n","                prompt += f\"\\n\\nPrevious chunk summary (context): {prev_summary}\"\n","\n","            content.append({\"type\": \"text\", \"text\": prompt})\n","            messages = [{\"role\": \"user\", \"content\": content}]\n","            t1 = time.perf_counter()\n","            print(f\"[{ts()}] step3 build_prompt_messages: {(t1 - t0)*1000:.1f} ms\")\n","\n","            t0 = time.perf_counter()\n","            inputs = processor.apply_chat_template(\n","                messages,\n","                tokenize=True,\n","                add_generation_prompt=True,\n","                return_dict=True,\n","                return_tensors=\"pt\",\n","            )\n","            t1 = time.perf_counter()\n","            print(f\"[{ts()}] step4 apply_chat_template(tokenize): {(t1 - t0)*1000:.1f} ms\")\n","\n","            t0 = time.perf_counter()\n","            inputs = {k: v.to(model.device) for k, v in inputs.items()}\n","            sync()\n","            t1 = time.perf_counter()\n","            print(f\"[{ts()}] step5 to_device(+sync): {(t1 - t0)*1000:.1f} ms\")\n","\n","            t0 = time.perf_counter()\n","            sync()\n","            with torch.inference_mode():\n","                out_ids = model.generate(\n","                    **inputs,\n","                    max_new_tokens=max_new_tokens,\n","                    do_sample=False,\n","                    num_beams=1,\n","                    repetition_penalty=1.05,\n","                )\n","            sync()\n","            t1 = time.perf_counter()\n","            print(f\"[{ts()}] step6 generate(+sync): {(t1 - t0):.3f} s\")\n","\n","            t0 = time.perf_counter()\n","            gen_ids = out_ids[:, inputs[\"input_ids\"].shape[1]:]\n","            text = processor.batch_decode(\n","                gen_ids,\n","                skip_special_tokens=True,\n","                clean_up_tokenization_spaces=False\n","            )[0].strip()\n","            t1 = time.perf_counter()\n","            print(f\"[{ts()}] step7 decode: {(t1 - t0)*1000:.1f} ms\")\n","\n","            print(f\"\\n=== Chunk {start_s}s ~ {end_s}s RESULT ===\")\n","            print(text)\n","\n","            prev_summary = text\n","            chunk_buffer = []\n","\n","            t_chunk1 = time.perf_counter()\n","            print(f\"[{ts()}] === Chunk total: {(t_chunk1 - t_chunk0):.3f} s ===\")\n","\n","    if chunk_buffer:\n","        t_chunk0 = time.perf_counter()\n","        start_s = current_frame_idx - len(chunk_buffer)\n","        end_s = current_frame_idx - 1\n","        print(f\"\\n[{ts()}] === Final Chunk {start_s}s ~ {end_s}s START ===\")\n","\n","        chunk = chunk_buffer\n","\n","        show_thumb_grid(chunk, scale=0.10, cols=4)\n","\n","        resized_chunk = []\n","        target_w = 512\n","        for im in chunk:\n","            w, h = im.size\n","            if w > target_w:\n","                new_h = int(h * (target_w / w))\n","                im = im.resize((target_w, new_h))\n","            resized_chunk.append(im)\n","\n","        chunk = resized_chunk\n","\n","        t0 = time.perf_counter()\n","        content = [{\"type\": \"image\", \"image\": im} for im in chunk]\n","        t1 = time.perf_counter()\n","        print(f\"[{ts()}] step2 build_image_content: {(t1 - t0)*1000:.1f} ms\")\n","\n","        t0 = time.perf_counter()\n","        prompt = (\n","            f\"These are frames sampled at 1 FPS from {start_s}s to {end_s}s.\\n\"\n","            \"Write a concise 2–3 sentence summary in English.\\n\"\n","            \"Do NOT output any reasoning, analysis, chain-of-thought, or step-by-step thinking.\\n\"\n","            \"Only output the final summary text.\\n\"\n","            \"If a new character appears, briefly describe appearance and actions.\\n\"\n","            \"If the character already appeared, refer to the prior description.\\n\"\n","            \"Keep it under 70 words.\"\n","        )\n","        if prev_summary:\n","            prompt += f\"\\n\\nPrevious chunk summary (context): {prev_summary}\"\n","\n","        content.append({\"type\": \"text\", \"text\": prompt})\n","        messages = [{\"role\": \"user\", \"content\": content}]\n","        t1 = time.perf_counter()\n","        print(f\"[{ts()}] step3 build_prompt_messages: {(t1 - t0)*1000:.1f} ms\")\n","\n","        t0 = time.perf_counter()\n","        inputs = processor.apply_chat_template(\n","            messages,\n","            tokenize=True,\n","            add_generation_prompt=True,\n","            return_dict=True,\n","            return_tensors=\"pt\",\n","        )\n","        t1 = time.perf_counter()\n","        print(f\"[{ts()}] step4 apply_chat_template(tokenize): {(t1 - t0)*1000:.1f} ms\")\n","\n","        t0 = time.perf_counter()\n","        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n","        sync()\n","        t1 = time.perf_counter()\n","        print(f\"[{ts()}] step5 to_device(+sync): {(t1 - t0)*1000:.1f} ms\")\n","\n","        t0 = time.perf_counter()\n","        sync()\n","        with torch.inference_mode():\n","            out_ids = model.generate(\n","                **inputs,\n","                max_new_tokens=max_new_tokens,\n","                do_sample=False,\n","                num_beams=1,\n","                repetition_penalty=1.05,\n","            )\n","        sync()\n","        t1 = time.perf_counter()\n","        print(f\"[{ts()}] step6 generate(+sync): {(t1 - t0):.3f} s\")\n","\n","        t0 = time.perf_counter()\n","        gen_ids = out_ids[:, inputs[\"input_ids\"].shape[1]:]\n","        text = processor.batch_decode(\n","            gen_ids,\n","            skip_special_tokens=True,\n","            clean_up_tokenization_spaces=False\n","        )[0].strip()\n","        t1 = time.perf_counter()\n","        print(f\"[{ts()}] step7 decode: {(t1 - t0)*1000:.1f} ms\")\n","\n","        print(f\"\\n=== Final Chunk {start_s}s ~ {end_s}s RESULT ===\")\n","        print(text)\n","\n","        prev_summary = text\n","\n","        t_chunk1 = time.perf_counter()\n","        print(f\"[{ts()}] === Final Chunk total: {(t_chunk1 - t_chunk0):.3f} s ===\")\n","\n","    t_all1 = time.perf_counter()\n","    print(f\"\\n[{ts()}] End total. Total elapsed: {(t_all1 - t_all0):.3f} s\")\n","\n","# --- Execution Code ---\n","video_path = \"sample01.mp4\"  # 700mb 파일 제너레이터 코드 실행 테스트\n","chunk_size = 8\n","max_new_tokens = 160\n","\n","frames_gen = frame_generator(video_path)\n","process_video_frames(frames_gen, chunk_size, max_new_tokens, processor, model)\n"],"metadata":{"id":"5IBb1SXpHsYx","colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1ZA_21NtysnSjx7zh8pyo7GY2KBh-RC3g"},"executionInfo":{"status":"ok","timestamp":1768367989788,"user_tz":-540,"elapsed":282860,"user":{"displayName":"다래","userId":"06334432749296783356"}},"outputId":"3f41338b-7c17-4090-bc26-d74cb959bffc"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","source":["\"\"\"\n","영상 크기가 큰 경우(700mb 이상) ram 절약하기 위해 개선한 코드임\n","\n","코드 수정한 프롬프트(제미나이 2.5 플래시):\n","마지막 셀 코드에 리스트에 frames을 리스트 저장이 아닌 yield 하고, 아래 for문도 generator를 인자값으로 받아서 실행할 수 있게 함수화 및 실행 코드로 바꿔줘, ram 절약 관점에서만 개선해주는 \"리팩토링\"이 되어야 해\n","\"\"\"\n","import cv2\n","from PIL import Image\n","from IPython.display import display\n","import time\n","from datetime import datetime\n","import torch\n","\n","# Helper functions\n","def ts():\n","    return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n","\n","def sync():\n","    if torch.cuda.is_available():\n","        torch.cuda.synchronize()\n","\n","\n","def frame_generator(video_path, max_seconds=None):\n","    cap = cv2.VideoCapture(video_path)\n","    if not cap.isOpened():\n","        raise RuntimeError(f\"Failed to open video: {video_path}\")\n","\n","    fps = cap.get(cv2.CAP_PROP_FPS)\n","    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","    duration_sec = int(total_frames / fps) if fps and fps > 0 else None\n","\n","    sec = 0\n","    while True:\n","        if max_seconds is not None and sec >= max_seconds:\n","            break\n","        if duration_sec is not None and sec >= duration_sec:\n","            break\n","\n","        cap.set(cv2.CAP_PROP_POS_MSEC, sec * 1000.0)\n","        ok, bgr = cap.read()\n","        if not ok:\n","            break\n","\n","        rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n","        yield Image.fromarray(rgb)\n","        sec += 1\n","    cap.release()\n","\n","def process_video_frames(frame_gen, chunk_size, max_new_tokens, processor, model):\n","    prev_summary = None\n","    t_all0 = time.perf_counter()\n","    print(f\"[{ts()}] Start total\")\n","\n","    chunk_buffer = []\n","    current_frame_idx = 0\n","\n","    for frame in frame_gen:\n","        chunk_buffer.append(frame)\n","        current_frame_idx += 1\n","\n","        if len(chunk_buffer) == chunk_size:\n","            t_chunk0 = time.perf_counter()\n","            start_s = current_frame_idx - chunk_size\n","            end_s = current_frame_idx - 1\n","            print(f\"\\n[{ts()}] === Chunk {start_s}s ~ {end_s}s START ===\")\n","\n","            chunk = chunk_buffer\n","\n","            show_thumb_grid(chunk, scale=0.10, cols=4)\n","\n","            resized_chunk = []\n","            target_w = 512\n","            for im in chunk:\n","                w, h = im.size\n","                if w > target_w:\n","                    new_h = int(h * (target_w / w))\n","                    im = im.resize((target_w, new_h))\n","                resized_chunk.append(im)\n","\n","            chunk = resized_chunk\n","\n","            t0 = time.perf_counter()\n","            content = [{\"type\": \"image\", \"image\": im} for im in chunk]\n","            t1 = time.perf_counter()\n","            print(f\"[{ts()}] step2 build_image_content: {(t1 - t0)*1000:.1f} ms\")\n","\n","            t0 = time.perf_counter()\n","            prompt = (\n","                f\"These are frames sampled at 1 FPS from {start_s}s to {end_s}s.\\n\"\n","                \"Write a concise 2–3 sentence summary in English.\\n\"\n","                \"Do NOT output any reasoning, analysis, chain-of-thought, or step-by-step thinking.\\n\"\n","                \"Only output the final summary text.\\n\"\n","                \"If a new character appears, briefly describe appearance and actions.\\n\"\n","                \"If the character already appeared, refer to the prior description.\\n\"\n","                \"Keep it under 70 words.\"\n","            )\n","            if prev_summary:\n","                prompt += f\"\\n\\nPrevious chunk summary (context): {prev_summary}\"\n","\n","            content.append({\"type\": \"text\", \"text\": prompt})\n","            messages = [{\"role\": \"user\", \"content\": content}]\n","            t1 = time.perf_counter()\n","            print(f\"[{ts()}] step3 build_prompt_messages: {(t1 - t0)*1000:.1f} ms\")\n","\n","            t0 = time.perf_counter()\n","            inputs = processor.apply_chat_template(\n","                messages,\n","                tokenize=True,\n","                add_generation_prompt=True,\n","                return_dict=True,\n","                return_tensors=\"pt\",\n","            )\n","            t1 = time.perf_counter()\n","            print(f\"[{ts()}] step4 apply_chat_template(tokenize): {(t1 - t0)*1000:.1f} ms\")\n","\n","            t0 = time.perf_counter()\n","            inputs = {k: v.to(model.device) for k, v in inputs.items()}\n","            sync()\n","            t1 = time.perf_counter()\n","            print(f\"[{ts()}] step5 to_device(+sync): {(t1 - t0)*1000:.1f} ms\")\n","\n","            t0 = time.perf_counter()\n","            sync()\n","            with torch.inference_mode():\n","                out_ids = model.generate(\n","                    **inputs,\n","                    max_new_tokens=max_new_tokens,\n","                    do_sample=False,\n","                    num_beams=1,\n","                    repetition_penalty=1.05,\n","                )\n","            sync()\n","            t1 = time.perf_counter()\n","            print(f\"[{ts()}] step6 generate(+sync): {(t1 - t0):.3f} s\")\n","\n","            t0 = time.perf_counter()\n","            gen_ids = out_ids[:, inputs[\"input_ids\"].shape[1]:]\n","            text = processor.batch_decode(\n","                gen_ids,\n","                skip_special_tokens=True,\n","                clean_up_tokenization_spaces=False\n","            )[0].strip()\n","            t1 = time.perf_counter()\n","            print(f\"[{ts()}] step7 decode: {(t1 - t0)*1000:.1f} ms\")\n","\n","            print(f\"\\n=== Chunk {start_s}s ~ {end_s}s RESULT ===\")\n","            print(text)\n","\n","            prev_summary = text\n","            chunk_buffer = []\n","\n","            t_chunk1 = time.perf_counter()\n","            print(f\"[{ts()}] === Chunk total: {(t_chunk1 - t_chunk0):.3f} s ===\")\n","\n","    if chunk_buffer:\n","        t_chunk0 = time.perf_counter()\n","        start_s = current_frame_idx - len(chunk_buffer)\n","        end_s = current_frame_idx - 1\n","        print(f\"\\n[{ts()}] === Final Chunk {start_s}s ~ {end_s}s START ===\")\n","\n","        chunk = chunk_buffer\n","\n","        show_thumb_grid(chunk, scale=0.10, cols=4)\n","\n","        resized_chunk = []\n","        target_w = 512\n","        for im in chunk:\n","            w, h = im.size\n","            if w > target_w:\n","                new_h = int(h * (target_w / w))\n","                im = im.resize((target_w, new_h))\n","            resized_chunk.append(im)\n","\n","        chunk = resized_chunk\n","\n","        t0 = time.perf_counter()\n","        content = [{\"type\": \"image\", \"image\": im} for im in chunk]\n","        t1 = time.perf_counter()\n","        print(f\"[{ts()}] step2 build_image_content: {(t1 - t0)*1000:.1f} ms\")\n","\n","        t0 = time.perf_counter()\n","        prompt = (\n","            f\"These are frames sampled at 1 FPS from {start_s}s to {end_s}s.\\n\"\n","            \"Write a concise 2–3 sentence summary in English.\\n\"\n","            \"Do NOT output any reasoning, analysis, chain-of-thought, or step-by-step thinking.\\n\"\n","            \"Only output the final summary text.\\n\"\n","            \"If a new character appears, briefly describe appearance and actions.\\n\"\n","            \"If the character already appeared, refer to the prior description.\\n\"\n","            \"Keep it under 70 words.\"\n","        )\n","        if prev_summary:\n","            prompt += f\"\\n\\nPrevious chunk summary (context): {prev_summary}\"\n","\n","        content.append({\"type\": \"text\", \"text\": prompt})\n","        messages = [{\"role\": \"user\", \"content\": content}]\n","        t1 = time.perf_counter()\n","        print(f\"[{ts()}] step3 build_prompt_messages: {(t1 - t0)*1000:.1f} ms\")\n","\n","        t0 = time.perf_counter()\n","        inputs = processor.apply_chat_template(\n","            messages,\n","            tokenize=True,\n","            add_generation_prompt=True,\n","            return_dict=True,\n","            return_tensors=\"pt\",\n","        )\n","        t1 = time.perf_counter()\n","        print(f\"[{ts()}] step4 apply_chat_template(tokenize): {(t1 - t0)*1000:.1f} ms\")\n","\n","        t0 = time.perf_counter()\n","        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n","        sync()\n","        t1 = time.perf_counter()\n","        print(f\"[{ts()}] step5 to_device(+sync): {(t1 - t0)*1000:.1f} ms\")\n","\n","        t0 = time.perf_counter()\n","        sync()\n","        with torch.inference_mode():\n","            out_ids = model.generate(\n","                **inputs,\n","                max_new_tokens=max_new_tokens,\n","                do_sample=False,\n","                num_beams=1,\n","                repetition_penalty=1.05,\n","            )\n","        sync()\n","        t1 = time.perf_counter()\n","        print(f\"[{ts()}] step6 generate(+sync): {(t1 - t0):.3f} s\")\n","\n","        t0 = time.perf_counter()\n","        gen_ids = out_ids[:, inputs[\"input_ids\"].shape[1]:]\n","        text = processor.batch_decode(\n","            gen_ids,\n","            skip_special_tokens=True,\n","            clean_up_tokenization_spaces=False\n","        )[0].strip()\n","        t1 = time.perf_counter()\n","        print(f\"[{ts()}] step7 decode: {(t1 - t0)*1000:.1f} ms\")\n","\n","        print(f\"\\n=== Final Chunk {start_s}s ~ {end_s}s RESULT ===\")\n","        print(text)\n","\n","        prev_summary = text\n","\n","        t_chunk1 = time.perf_counter()\n","        print(f\"[{ts()}] === Final Chunk total: {(t_chunk1 - t_chunk0):.3f} s ===\")\n","\n","    t_all1 = time.perf_counter()\n","    print(f\"\\n[{ts()}] End total. Total elapsed: {(t_all1 - t_all0):.3f} s\")\n","\n","# --- Execution Code ---\n","video_path = \"sample02.mp4\"  # 700mb 파일 제너레이터 코드 실행 테스트\n","chunk_size = 8\n","max_new_tokens = 160\n","\n","frames_gen = frame_generator(video_path)\n","process_video_frames(frames_gen, chunk_size, max_new_tokens, processor, model)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1uALFPKal5LqoAGZ9yj-bxIxq9VJBQ3vR"},"id":"il6HGj0s5t8k","executionInfo":{"status":"ok","timestamp":1768368490889,"user_tz":-540,"elapsed":275774,"user":{"displayName":"다래","userId":"06334432749296783356"}},"outputId":"5f0f2c4d-52c0-4aad-91c6-2d4048d6eb2a"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","source":["\"\"\"\n","영상 크기가 큰 경우(700mb 이상) ram 절약하기 위해 개선한 코드임\n","\n","코드 수정한 프롬프트(제미나이 2.5 플래시):\n","마지막 셀 코드에 리스트에 frames을 리스트 저장이 아닌 yield 하고, 아래 for문도 generator를 인자값으로 받아서 실행할 수 있게 함수화 및 실행 코드로 바꿔줘, ram 절약 관점에서만 개선해주는 \"리팩토링\"이 되어야 해\n","\"\"\"\n","import cv2\n","from PIL import Image\n","from IPython.display import display\n","import time\n","from datetime import datetime\n","import torch\n","\n","# Helper functions\n","def ts():\n","    return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n","\n","def sync():\n","    if torch.cuda.is_available():\n","        torch.cuda.synchronize()\n","\n","\n","def frame_generator(video_path, max_seconds=None):\n","    cap = cv2.VideoCapture(video_path)\n","    if not cap.isOpened():\n","        raise RuntimeError(f\"Failed to open video: {video_path}\")\n","\n","    fps = cap.get(cv2.CAP_PROP_FPS)\n","    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","    duration_sec = int(total_frames / fps) if fps and fps > 0 else None\n","\n","    sec = 0\n","    while True:\n","        if max_seconds is not None and sec >= max_seconds:\n","            break\n","        if duration_sec is not None and sec >= duration_sec:\n","            break\n","\n","        cap.set(cv2.CAP_PROP_POS_MSEC, sec * 1000.0)\n","        ok, bgr = cap.read()\n","        if not ok:\n","            break\n","\n","        rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n","        yield Image.fromarray(rgb)\n","        sec += 1\n","    cap.release()\n","\n","def process_video_frames(frame_gen, chunk_size, max_new_tokens, processor, model):\n","    prev_summary = None\n","    t_all0 = time.perf_counter()\n","    print(f\"[{ts()}] Start total\")\n","\n","    chunk_buffer = []\n","    current_frame_idx = 0\n","\n","    for frame in frame_gen:\n","        chunk_buffer.append(frame)\n","        current_frame_idx += 1\n","\n","        if len(chunk_buffer) == chunk_size:\n","            t_chunk0 = time.perf_counter()\n","            start_s = current_frame_idx - chunk_size\n","            end_s = current_frame_idx - 1\n","            print(f\"\\n[{ts()}] === Chunk {start_s}s ~ {end_s}s START ===\")\n","\n","            chunk = chunk_buffer\n","\n","            show_thumb_grid(chunk, scale=0.10, cols=4)\n","\n","            resized_chunk = []\n","            target_w = 512\n","            for im in chunk:\n","                w, h = im.size\n","                if w > target_w:\n","                    new_h = int(h * (target_w / w))\n","                    im = im.resize((target_w, new_h))\n","                resized_chunk.append(im)\n","\n","            chunk = resized_chunk\n","\n","            t0 = time.perf_counter()\n","            content = [{\"type\": \"image\", \"image\": im} for im in chunk]\n","            t1 = time.perf_counter()\n","            print(f\"[{ts()}] step2 build_image_content: {(t1 - t0)*1000:.1f} ms\")\n","\n","            t0 = time.perf_counter()\n","            prompt = (\n","                f\"These are frames sampled at 1 FPS from {start_s}s to {end_s}s.\\n\"\n","                \"Write a concise 2–3 sentence summary in English.\\n\"\n","                \"Do NOT output any reasoning, analysis, chain-of-thought, or step-by-step thinking.\\n\"\n","                \"Only output the final summary text.\\n\"\n","                \"If a new character appears, briefly describe appearance and actions.\\n\"\n","                \"If the character already appeared, refer to the prior description.\\n\"\n","                \"Keep it under 70 words.\"\n","            )\n","            if prev_summary:\n","                prompt += f\"\\n\\nPrevious chunk summary (context): {prev_summary}\"\n","\n","            content.append({\"type\": \"text\", \"text\": prompt})\n","            messages = [{\"role\": \"user\", \"content\": content}]\n","            t1 = time.perf_counter()\n","            print(f\"[{ts()}] step3 build_prompt_messages: {(t1 - t0)*1000:.1f} ms\")\n","\n","            t0 = time.perf_counter()\n","            inputs = processor.apply_chat_template(\n","                messages,\n","                tokenize=True,\n","                add_generation_prompt=True,\n","                return_dict=True,\n","                return_tensors=\"pt\",\n","            )\n","            t1 = time.perf_counter()\n","            print(f\"[{ts()}] step4 apply_chat_template(tokenize): {(t1 - t0)*1000:.1f} ms\")\n","\n","            t0 = time.perf_counter()\n","            inputs = {k: v.to(model.device) for k, v in inputs.items()}\n","            sync()\n","            t1 = time.perf_counter()\n","            print(f\"[{ts()}] step5 to_device(+sync): {(t1 - t0)*1000:.1f} ms\")\n","\n","            t0 = time.perf_counter()\n","            sync()\n","            with torch.inference_mode():\n","                out_ids = model.generate(\n","                    **inputs,\n","                    max_new_tokens=max_new_tokens,\n","                    do_sample=False,\n","                    num_beams=1,\n","                    repetition_penalty=1.05,\n","                )\n","            sync()\n","            t1 = time.perf_counter()\n","            print(f\"[{ts()}] step6 generate(+sync): {(t1 - t0):.3f} s\")\n","\n","            t0 = time.perf_counter()\n","            gen_ids = out_ids[:, inputs[\"input_ids\"].shape[1]:]\n","            text = processor.batch_decode(\n","                gen_ids,\n","                skip_special_tokens=True,\n","                clean_up_tokenization_spaces=False\n","            )[0].strip()\n","            t1 = time.perf_counter()\n","            print(f\"[{ts()}] step7 decode: {(t1 - t0)*1000:.1f} ms\")\n","\n","            print(f\"\\n=== Chunk {start_s}s ~ {end_s}s RESULT ===\")\n","            print(text)\n","\n","            prev_summary = text\n","            chunk_buffer = []\n","\n","            t_chunk1 = time.perf_counter()\n","            print(f\"[{ts()}] === Chunk total: {(t_chunk1 - t_chunk0):.3f} s ===\")\n","\n","    if chunk_buffer:\n","        t_chunk0 = time.perf_counter()\n","        start_s = current_frame_idx - len(chunk_buffer)\n","        end_s = current_frame_idx - 1\n","        print(f\"\\n[{ts()}] === Final Chunk {start_s}s ~ {end_s}s START ===\")\n","\n","        chunk = chunk_buffer\n","\n","        show_thumb_grid(chunk, scale=0.10, cols=4)\n","\n","        resized_chunk = []\n","        target_w = 512\n","        for im in chunk:\n","            w, h = im.size\n","            if w > target_w:\n","                new_h = int(h * (target_w / w))\n","                im = im.resize((target_w, new_h))\n","            resized_chunk.append(im)\n","\n","        chunk = resized_chunk\n","\n","        t0 = time.perf_counter()\n","        content = [{\"type\": \"image\", \"image\": im} for im in chunk]\n","        t1 = time.perf_counter()\n","        print(f\"[{ts()}] step2 build_image_content: {(t1 - t0)*1000:.1f} ms\")\n","\n","        t0 = time.perf_counter()\n","        prompt = (\n","            f\"These are frames sampled at 1 FPS from {start_s}s to {end_s}s.\\n\"\n","            \"Write a concise 2–3 sentence summary in English.\\n\"\n","            \"Do NOT output any reasoning, analysis, chain-of-thought, or step-by-step thinking.\\n\"\n","            \"Only output the final summary text.\\n\"\n","            \"If a new character appears, briefly describe appearance and actions.\\n\"\n","            \"If the character already appeared, refer to the prior description.\\n\"\n","            \"Keep it under 70 words.\"\n","        )\n","        if prev_summary:\n","            prompt += f\"\\n\\nPrevious chunk summary (context): {prev_summary}\"\n","\n","        content.append({\"type\": \"text\", \"text\": prompt})\n","        messages = [{\"role\": \"user\", \"content\": content}]\n","        t1 = time.perf_counter()\n","        print(f\"[{ts()}] step3 build_prompt_messages: {(t1 - t0)*1000:.1f} ms\")\n","\n","        t0 = time.perf_counter()\n","        inputs = processor.apply_chat_template(\n","            messages,\n","            tokenize=True,\n","            add_generation_prompt=True,\n","            return_dict=True,\n","            return_tensors=\"pt\",\n","        )\n","        t1 = time.perf_counter()\n","        print(f\"[{ts()}] step4 apply_chat_template(tokenize): {(t1 - t0)*1000:.1f} ms\")\n","\n","        t0 = time.perf_counter()\n","        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n","        sync()\n","        t1 = time.perf_counter()\n","        print(f\"[{ts()}] step5 to_device(+sync): {(t1 - t0)*1000:.1f} ms\")\n","\n","        t0 = time.perf_counter()\n","        sync()\n","        with torch.inference_mode():\n","            out_ids = model.generate(\n","                **inputs,\n","                max_new_tokens=max_new_tokens,\n","                do_sample=False,\n","                num_beams=1,\n","                repetition_penalty=1.05,\n","            )\n","        sync()\n","        t1 = time.perf_counter()\n","        print(f\"[{ts()}] step6 generate(+sync): {(t1 - t0):.3f} s\")\n","\n","        t0 = time.perf_counter()\n","        gen_ids = out_ids[:, inputs[\"input_ids\"].shape[1]:]\n","        text = processor.batch_decode(\n","            gen_ids,\n","            skip_special_tokens=True,\n","            clean_up_tokenization_spaces=False\n","        )[0].strip()\n","        t1 = time.perf_counter()\n","        print(f\"[{ts()}] step7 decode: {(t1 - t0)*1000:.1f} ms\")\n","\n","        print(f\"\\n=== Final Chunk {start_s}s ~ {end_s}s RESULT ===\")\n","        print(text)\n","\n","        prev_summary = text\n","\n","        t_chunk1 = time.perf_counter()\n","        print(f\"[{ts()}] === Final Chunk total: {(t_chunk1 - t_chunk0):.3f} s ===\")\n","\n","    t_all1 = time.perf_counter()\n","    print(f\"\\n[{ts()}] End total. Total elapsed: {(t_all1 - t_all0):.3f} s\")\n","\n","# --- Execution Code ---\n","video_path = \"sample03.mp4\"  # 700mb 파일 제너레이터 코드 실행 테스트\n","chunk_size = 8\n","max_new_tokens = 160\n","\n","frames_gen = frame_generator(video_path)\n","process_video_frames(frames_gen, chunk_size, max_new_tokens, processor, model)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1y9t98SoRnDKdgfapyq1GxOHOYdMtR_gG"},"id":"qc4O73Wf52s5","executionInfo":{"status":"ok","timestamp":1768369220849,"user_tz":-540,"elapsed":510742,"user":{"displayName":"다래","userId":"06334432749296783356"}},"outputId":"7097402b-1cf9-47a6-be19-8655b72f2504"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","source":["\"\"\"\n","영상 크기가 큰 경우(700mb 이상) ram 절약하기 위해 개선한 코드임\n","\n","코드 수정한 프롬프트(제미나이 2.5 플래시):\n","마지막 셀 코드에 리스트에 frames을 리스트 저장이 아닌 yield 하고, 아래 for문도 generator를 인자값으로 받아서 실행할 수 있게 함수화 및 실행 코드로 바꿔줘, ram 절약 관점에서만 개선해주는 \"리팩토링\"이 되어야 해\n","\"\"\"\n","import cv2\n","from PIL import Image\n","from IPython.display import display\n","import time\n","from datetime import datetime\n","import torch\n","\n","# Helper functions\n","def ts():\n","    return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n","\n","def sync():\n","    if torch.cuda.is_available():\n","        torch.cuda.synchronize()\n","\n","\n","def frame_generator(video_path, max_seconds=None):\n","    cap = cv2.VideoCapture(video_path)\n","    if not cap.isOpened():\n","        raise RuntimeError(f\"Failed to open video: {video_path}\")\n","\n","    fps = cap.get(cv2.CAP_PROP_FPS)\n","    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","    duration_sec = int(total_frames / fps) if fps and fps > 0 else None\n","\n","    sec = 0\n","    while True:\n","        if max_seconds is not None and sec >= max_seconds:\n","            break\n","        if duration_sec is not None and sec >= duration_sec:\n","            break\n","\n","        cap.set(cv2.CAP_PROP_POS_MSEC, sec * 1000.0)\n","        ok, bgr = cap.read()\n","        if not ok:\n","            break\n","\n","        rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n","        yield Image.fromarray(rgb)\n","        sec += 1\n","    cap.release()\n","\n","def process_video_frames(frame_gen, chunk_size, max_new_tokens, processor, model):\n","    prev_summary = None\n","    t_all0 = time.perf_counter()\n","    print(f\"[{ts()}] Start total\")\n","\n","    chunk_buffer = []\n","    current_frame_idx = 0\n","\n","    for frame in frame_gen:\n","        chunk_buffer.append(frame)\n","        current_frame_idx += 1\n","\n","        if len(chunk_buffer) == chunk_size:\n","            t_chunk0 = time.perf_counter()\n","            start_s = current_frame_idx - chunk_size\n","            end_s = current_frame_idx - 1\n","            print(f\"\\n[{ts()}] === Chunk {start_s}s ~ {end_s}s START ===\")\n","\n","            chunk = chunk_buffer\n","\n","            show_thumb_grid(chunk, scale=0.10, cols=4)\n","\n","            resized_chunk = []\n","            target_w = 512\n","            for im in chunk:\n","                w, h = im.size\n","                if w > target_w:\n","                    new_h = int(h * (target_w / w))\n","                    im = im.resize((target_w, new_h))\n","                resized_chunk.append(im)\n","\n","            chunk = resized_chunk\n","\n","            t0 = time.perf_counter()\n","            content = [{\"type\": \"image\", \"image\": im} for im in chunk]\n","            t1 = time.perf_counter()\n","            print(f\"[{ts()}] step2 build_image_content: {(t1 - t0)*1000:.1f} ms\")\n","\n","            t0 = time.perf_counter()\n","            prompt = (\n","                f\"These are frames sampled at 1 FPS from {start_s}s to {end_s}s.\\n\"\n","                \"Write a concise 2–3 sentence summary in English.\\n\"\n","                \"Do NOT output any reasoning, analysis, chain-of-thought, or step-by-step thinking.\\n\"\n","                \"Only output the final summary text.\\n\"\n","                \"If a new character appears, briefly describe appearance and actions.\\n\"\n","                \"If the character already appeared, refer to the prior description.\\n\"\n","                \"Keep it under 70 words.\"\n","            )\n","            if prev_summary:\n","                prompt += f\"\\n\\nPrevious chunk summary (context): {prev_summary}\"\n","\n","            content.append({\"type\": \"text\", \"text\": prompt})\n","            messages = [{\"role\": \"user\", \"content\": content}]\n","            t1 = time.perf_counter()\n","            print(f\"[{ts()}] step3 build_prompt_messages: {(t1 - t0)*1000:.1f} ms\")\n","\n","            t0 = time.perf_counter()\n","            inputs = processor.apply_chat_template(\n","                messages,\n","                tokenize=True,\n","                add_generation_prompt=True,\n","                return_dict=True,\n","                return_tensors=\"pt\",\n","            )\n","            t1 = time.perf_counter()\n","            print(f\"[{ts()}] step4 apply_chat_template(tokenize): {(t1 - t0)*1000:.1f} ms\")\n","\n","            t0 = time.perf_counter()\n","            inputs = {k: v.to(model.device) for k, v in inputs.items()}\n","            sync()\n","            t1 = time.perf_counter()\n","            print(f\"[{ts()}] step5 to_device(+sync): {(t1 - t0)*1000:.1f} ms\")\n","\n","            t0 = time.perf_counter()\n","            sync()\n","            with torch.inference_mode():\n","                out_ids = model.generate(\n","                    **inputs,\n","                    max_new_tokens=max_new_tokens,\n","                    do_sample=False,\n","                    num_beams=1,\n","                    repetition_penalty=1.05,\n","                )\n","            sync()\n","            t1 = time.perf_counter()\n","            print(f\"[{ts()}] step6 generate(+sync): {(t1 - t0):.3f} s\")\n","\n","            t0 = time.perf_counter()\n","            gen_ids = out_ids[:, inputs[\"input_ids\"].shape[1]:]\n","            text = processor.batch_decode(\n","                gen_ids,\n","                skip_special_tokens=True,\n","                clean_up_tokenization_spaces=False\n","            )[0].strip()\n","            t1 = time.perf_counter()\n","            print(f\"[{ts()}] step7 decode: {(t1 - t0)*1000:.1f} ms\")\n","\n","            print(f\"\\n=== Chunk {start_s}s ~ {end_s}s RESULT ===\")\n","            print(text)\n","\n","            prev_summary = text\n","            chunk_buffer = []\n","\n","            t_chunk1 = time.perf_counter()\n","            print(f\"[{ts()}] === Chunk total: {(t_chunk1 - t_chunk0):.3f} s ===\")\n","\n","    if chunk_buffer:\n","        t_chunk0 = time.perf_counter()\n","        start_s = current_frame_idx - len(chunk_buffer)\n","        end_s = current_frame_idx - 1\n","        print(f\"\\n[{ts()}] === Final Chunk {start_s}s ~ {end_s}s START ===\")\n","\n","        chunk = chunk_buffer\n","\n","        show_thumb_grid(chunk, scale=0.10, cols=4)\n","\n","        resized_chunk = []\n","        target_w = 512\n","        for im in chunk:\n","            w, h = im.size\n","            if w > target_w:\n","                new_h = int(h * (target_w / w))\n","                im = im.resize((target_w, new_h))\n","            resized_chunk.append(im)\n","\n","        chunk = resized_chunk\n","\n","        t0 = time.perf_counter()\n","        content = [{\"type\": \"image\", \"image\": im} for im in chunk]\n","        t1 = time.perf_counter()\n","        print(f\"[{ts()}] step2 build_image_content: {(t1 - t0)*1000:.1f} ms\")\n","\n","        t0 = time.perf_counter()\n","        prompt = (\n","            f\"These are frames sampled at 1 FPS from {start_s}s to {end_s}s.\\n\"\n","            \"Write a concise 2–3 sentence summary in English.\\n\"\n","            \"Do NOT output any reasoning, analysis, chain-of-thought, or step-by-step thinking.\\n\"\n","            \"Only output the final summary text.\\n\"\n","            \"If a new character appears, briefly describe appearance and actions.\\n\"\n","            \"If the character already appeared, refer to the prior description.\\n\"\n","            \"Keep it under 70 words.\"\n","        )\n","        if prev_summary:\n","            prompt += f\"\\n\\nPrevious chunk summary (context): {prev_summary}\"\n","\n","        content.append({\"type\": \"text\", \"text\": prompt})\n","        messages = [{\"role\": \"user\", \"content\": content}]\n","        t1 = time.perf_counter()\n","        print(f\"[{ts()}] step3 build_prompt_messages: {(t1 - t0)*1000:.1f} ms\")\n","\n","        t0 = time.perf_counter()\n","        inputs = processor.apply_chat_template(\n","            messages,\n","            tokenize=True,\n","            add_generation_prompt=True,\n","            return_dict=True,\n","            return_tensors=\"pt\",\n","        )\n","        t1 = time.perf_counter()\n","        print(f\"[{ts()}] step4 apply_chat_template(tokenize): {(t1 - t0)*1000:.1f} ms\")\n","\n","        t0 = time.perf_counter()\n","        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n","        sync()\n","        t1 = time.perf_counter()\n","        print(f\"[{ts()}] step5 to_device(+sync): {(t1 - t0)*1000:.1f} ms\")\n","\n","        t0 = time.perf_counter()\n","        sync()\n","        with torch.inference_mode():\n","            out_ids = model.generate(\n","                **inputs,\n","                max_new_tokens=max_new_tokens,\n","                do_sample=False,\n","                num_beams=1,\n","                repetition_penalty=1.05,\n","            )\n","        sync()\n","        t1 = time.perf_counter()\n","        print(f\"[{ts()}] step6 generate(+sync): {(t1 - t0):.3f} s\")\n","\n","        t0 = time.perf_counter()\n","        gen_ids = out_ids[:, inputs[\"input_ids\"].shape[1]:]\n","        text = processor.batch_decode(\n","            gen_ids,\n","            skip_special_tokens=True,\n","            clean_up_tokenization_spaces=False\n","        )[0].strip()\n","        t1 = time.perf_counter()\n","        print(f\"[{ts()}] step7 decode: {(t1 - t0)*1000:.1f} ms\")\n","\n","        print(f\"\\n=== Final Chunk {start_s}s ~ {end_s}s RESULT ===\")\n","        print(text)\n","\n","        prev_summary = text\n","\n","        t_chunk1 = time.perf_counter()\n","        print(f\"[{ts()}] === Final Chunk total: {(t_chunk1 - t_chunk0):.3f} s ===\")\n","\n","    t_all1 = time.perf_counter()\n","    print(f\"\\n[{ts()}] End total. Total elapsed: {(t_all1 - t_all0):.3f} s\")\n","\n","# --- Execution Code ---\n","video_path = \"sample04.mp4\"  # 700mb 파일 제너레이터 코드 실행 테스트\n","chunk_size = 8\n","max_new_tokens = 160\n","\n","frames_gen = frame_generator(video_path)\n","process_video_frames(frames_gen, chunk_size, max_new_tokens, processor, model)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1flLaLcM5-NHF55Kcj86CBVy4YMJ8IoFO"},"id":"G8chs3GK53Nz","executionInfo":{"status":"ok","timestamp":1768369824500,"user_tz":-540,"elapsed":603281,"user":{"displayName":"다래","userId":"06334432749296783356"}},"outputId":"c89d6412-bf1f-4950-8ee8-f604337082b1"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]}]}